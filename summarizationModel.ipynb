{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566},{"sourceId":13559864,"sourceType":"datasetVersion","datasetId":8613108}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate sentencepiece accelerate rouge_score streamlit gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T19:38:13.946011Z","iopub.execute_input":"2025-10-30T19:38:13.946733Z","iopub.status.idle":"2025-10-30T19:40:00.093517Z","shell.execute_reply.started":"2025-10-30T19:38:13.946707Z","shell.execute_reply":"2025-10-30T19:40:00.092777Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\nimport os\nimport random\nimport torch\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq,\n)\nimport evaluate\n\n\nMODEL_NAME = \"t5-base\"  \nOUTPUT_DIR = \"models/t5_summarization\"\nMAX_INPUT_LENGTH = 512\nMAX_TARGET_LENGTH = 128\nTRAIN_BATCH = 4\nEVAL_BATCH = 4\nNUM_EPOCHS = 3\nLR = 5e-5\nSEED = 42\nDATASET_SAMPLE_SIZE = 0.05  \n\nTRAIN_CSV = \"/kaggle/input/cnndata/train.csv\"\nVAL_CSV = \"/kaggle/input/cnndata/validation.csv\"\nTEST_CSV = \"/kaggle/input/cnndata/test.csv\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:18:45.112820Z","iopub.execute_input":"2025-10-30T20:18:45.113614Z","iopub.status.idle":"2025-10-30T20:18:45.120707Z","shell.execute_reply.started":"2025-10-30T20:18:45.113585Z","shell.execute_reply":"2025-10-30T20:18:45.120039Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nfor path in [TRAIN_CSV, VAL_CSV, TEST_CSV]:\n    print(\"Path:\", path, \"Exists?\", os.path.exists(path))\n\ntrain_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\ntest_df  = pd.read_csv(TEST_CSV)\n\nprint(\"\\nTrain shape:\", train_df.shape)\nprint(\"Validation shape:\", val_df.shape)\nprint(\"Test shape:\", test_df.shape)\n\nprint(\"\\nTrain columns:\", train_df.columns.tolist())\ntrain_df.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:19:25.192944Z","iopub.execute_input":"2025-10-30T20:19:25.193207Z","iopub.status.idle":"2025-10-30T20:19:39.018453Z","shell.execute_reply.started":"2025-10-30T20:19:25.193188Z","shell.execute_reply":"2025-10-30T20:19:39.017645Z"}},"outputs":[{"name":"stdout","text":"Path: /kaggle/input/cnndata/train.csv Exists? True\nPath: /kaggle/input/cnndata/validation.csv Exists? True\nPath: /kaggle/input/cnndata/test.csv Exists? True\n\nTrain shape: (287113, 3)\nValidation shape: (13368, 3)\nTest shape: (11490, 3)\n\nTrain columns: ['id', 'article', 'highlights']\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                         id  \\\n0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n2  00027e965c8264c35cc1bc55556db388da82b07f   \n\n                                             article  \\\n0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n1  (CNN) -- Ralph Mata was an internal affairs li...   \n2  A drunk driver who killed a young woman in a h...   \n\n                                          highlights  \n0  Bishop John Folda, of North Dakota, is taking ...  \n1  Criminal complaint: Cop used his role to help ...  \n2  Craig Eccleston-Todd, 27, had drunk at least t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n      <td>Criminal complaint: Cop used his role to help ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n      <td>A drunk driver who killed a young woman in a h...</td>\n      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\ndef df_to_datasetdict(train_df, val_df, test_df):\n    ds_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n    ds_val   = Dataset.from_pandas(val_df.reset_index(drop=True))\n    ds_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n    return DatasetDict({\"train\": ds_train, \"validation\": ds_val, \"test\": ds_test})\n\ndataset = df_to_datasetdict(train_df, val_df, test_df)\n\nprint(\"Original sizes:\")\nfor s, d in dataset.items():\n    print(f\"  {s}: {len(d)}\")\n\ndef sample_dataset(dataset: DatasetDict, frac: float, seed: int = SEED):\n    if frac >= 1.0:\n        return dataset\n    sampled = {}\n    for split, ds in dataset.items():\n        n = max(1, int(len(ds) * frac))\n        sampled[split] = ds.shuffle(seed=seed).select(range(n))\n        print(f\"Sampled {len(sampled[split])}/{len(ds)} from {split}\")\n    return DatasetDict(sampled)\n\ndataset = sample_dataset(dataset, DATASET_SAMPLE_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:19:53.788459Z","iopub.execute_input":"2025-10-30T20:19:53.788997Z","iopub.status.idle":"2025-10-30T20:20:05.463778Z","shell.execute_reply.started":"2025-10-30T20:19:53.788976Z","shell.execute_reply":"2025-10-30T20:20:05.463076Z"}},"outputs":[{"name":"stdout","text":"Original sizes:\n  train: 287113\n  validation: 13368\n  test: 11490\nSampled 14355/287113 from train\nSampled 668/13368 from validation\nSampled 574/11490 from test\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def preprocess_examples(examples, tokenizer, model_name=MODEL_NAME):\n    inputs = examples.get(\"article\") or examples.get(\"text\") or examples.get(\"content\")\n    targets = examples.get(\"highlights\") or examples.get(\"summary\") or examples.get(\"target\")\n    if inputs is None or targets is None:\n        raise ValueError(\"error\")\n    if \"t5\" in model_name.lower():\n        inputs = [\"summarize: \" + str(x).strip() for x in inputs]\n    else:\n        inputs = [str(x).strip() for x in inputs]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n    labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:20:24.324427Z","iopub.execute_input":"2025-10-30T20:20:24.325071Z","iopub.status.idle":"2025-10-30T20:20:24.330366Z","shell.execute_reply.started":"2025-10-30T20:20:24.325047Z","shell.execute_reply":"2025-10-30T20:20:24.329633Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\nprint(\"Loaded:\", MODEL_NAME)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:20:55.564867Z","iopub.execute_input":"2025-10-30T20:20:55.565562Z","iopub.status.idle":"2025-10-30T20:21:01.229101Z","shell.execute_reply.started":"2025-10-30T20:20:55.565536Z","shell.execute_reply":"2025-10-30T20:21:01.228281Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32272c114633451ba0cdcaf03ad0ec6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97dc5ceb9c34eb3999510dd192f8ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a5bad6d5164545b74d672e74bb348d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ac803056db4deca032908e037017d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e000c3a8f30446068e4c5338fd8c35ed"}},"metadata":{}},{"name":"stdout","text":"Loaded: t5-base\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"tokenized_datasets = dataset.map(\n    lambda examples: preprocess_examples(examples, tokenizer),\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    desc=\"Tokenizing dataset\",\n)\n\nprint(tokenized_datasets)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:21:14.427146Z","iopub.execute_input":"2025-10-30T20:21:14.427700Z","iopub.status.idle":"2025-10-30T20:21:39.599241Z","shell.execute_reply.started":"2025-10-30T20:21:14.427674Z","shell.execute_reply":"2025-10-30T20:21:39.598487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/14355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e01c90fd04746ba81317d45fcde5dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"631bc67d5443464dac01a93243e1f971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"717aab94bf4e4238889bce08dcda1d0b"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14355\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 668\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 574\n    })\n})\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\nrouge = evaluate.load(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [l.strip() for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in lab] for lab in labels]\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result = {k: round(v, 4) for k, v in result.items()}\n    gen_lens = [len(tokenizer.encode(p)) for p in decoded_preds]\n    result[\"gen_len\"] = round(sum(gen_lens) / max(1, len(gen_lens)), 2)\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:22:10.512854Z","iopub.execute_input":"2025-10-30T20:22:10.513141Z","iopub.status.idle":"2025-10-30T20:22:12.256347Z","shell.execute_reply.started":"2025-10-30T20:22:10.513119Z","shell.execute_reply":"2025-10-30T20:22:12.255569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb39985e6f24751bddd7ad0cb3239b9"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"steps\",           \n    eval_steps=500 if len(tokenized_datasets[\"train\"]) > 500 else 100,\n    save_strategy=\"steps\",\n    save_steps=500 if len(tokenized_datasets[\"train\"]) > 500 else 100,\n    per_device_train_batch_size=TRAIN_BATCH,\n    per_device_eval_batch_size=EVAL_BATCH,\n    predict_with_generate=True,\n    num_train_epochs=NUM_EPOCHS,\n    learning_rate=LR,\n    logging_steps=100,\n    save_total_limit=2,\n    fp16=torch.cuda.is_available(),\n    gradient_accumulation_steps=4,\n    seed=SEED,\n    report_to=[],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rougeL\",\n    greater_is_better=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Trainer initialized successfully ✅\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:25:17.387606Z","iopub.execute_input":"2025-10-30T20:25:17.387872Z","iopub.status.idle":"2025-10-30T20:25:17.431624Z","shell.execute_reply.started":"2025-10-30T20:25:17.387854Z","shell.execute_reply":"2025-10-30T20:25:17.430704Z"}},"outputs":[{"name":"stdout","text":"Trainer initialized successfully ✅\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/4205080032.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"train_result = trainer.train()\ntrainer.save_model(OUTPUT_DIR)\nprint(\"Training finished. Model saved to:\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T20:26:55.840875Z","iopub.execute_input":"2025-10-30T20:26:55.841457Z","iopub.status.idle":"2025-10-30T21:47:48.238830Z","shell.execute_reply.started":"2025-10-30T20:26:55.841434Z","shell.execute_reply":"2025-10-30T21:47:48.237902Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1347' max='1347' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1347/1347 1:20:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.557100</td>\n      <td>1.579499</td>\n      <td>0.252800</td>\n      <td>0.121300</td>\n      <td>0.207000</td>\n      <td>0.207200</td>\n      <td>20.870000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.509900</td>\n      <td>1.577270</td>\n      <td>0.253900</td>\n      <td>0.122900</td>\n      <td>0.209200</td>\n      <td>0.209300</td>\n      <td>20.870000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Training finished. Model saved to: models/t5_summarization\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\nprint(\"Test metrics (ROUGE):\")\nfor k, v in test_metrics.items():\n    print(f\"  {k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T21:50:54.170496Z","iopub.execute_input":"2025-10-30T21:50:54.171204Z","iopub.status.idle":"2025-10-30T21:52:15.809705Z","shell.execute_reply.started":"2025-10-30T21:50:54.171180Z","shell.execute_reply":"2025-10-30T21:52:15.808954Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [72/72 01:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test metrics (ROUGE):\n  eval_loss: 1.5522174835205078\n  eval_rouge1: 0.2685\n  eval_rouge2: 0.1309\n  eval_rougeL: 0.2206\n  eval_rougeLsum: 0.2206\n  eval_gen_len: 20.87\n  eval_runtime: 81.6292\n  eval_samples_per_second: 7.032\n  eval_steps_per_second: 0.882\n  epoch: 3.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_PATH = OUTPUT_DIR\nprint(\"Loading model...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\nprint(\"Model ready ✅\")\n\ndef generate_summary(text, max_length, min_length, beam_width):\n    if not text.strip():\n        return \"Please enter some text to summarize.\"\n    input_text = \"summarize: \" + text.strip()\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_length=max_length,\n            min_length=min_length,\n            num_beams=beam_width,\n            early_stopping=True,\n            no_repeat_ngram_size=3,\n            length_penalty=2.0\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nwith gr.Blocks(title=\"Text Summarizer\") as demo:\n    gr.Markdown(\"# ✏️ Text Summarization Demo\")\n    gr.Markdown(\"Fine-tuned T5/BART model that generates short summaries from long text.\")\n    with gr.Row():\n        with gr.Column(scale=3):\n            input_text = gr.Textbox(label=\"Enter your text:\", lines=10, placeholder=\"Paste your article here...\")\n            summarize_btn = gr.Button(\"Generate Summary\")\n            output_summary = gr.Textbox(label=\"Summary:\", lines=6)\n            gr.Examples(\n                examples=[\n                    [\"The global economy is facing challenges due to rising inflation and policy changes.\"],\n                    [\"AI is transforming industries, healthcare, and automation worldwide.\"],\n                    [\"Pakistan won the cricket match after a thrilling chase led by Babar Azam.\"]\n                ],\n                inputs=input_text\n            )\n        with gr.Column(scale=1):\n            gr.Markdown(\"### Settings\")\n            max_length = gr.Slider(50, 200, value=100, step=10, label=\"Max Length\")\n            min_length = gr.Slider(10, 60, value=20, step=5, label=\"Min Length\")\n            beam_width = gr.Slider(1, 6, value=4, step=1, label=\"Beam Width\")\n    summarize_btn.click(fn=generate_summary, inputs=[input_text, max_length, min_length, beam_width], outputs=output_summary)\n\nprint(\"\\nLaunching Gradio App...\")\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T22:31:30.897602Z","iopub.execute_input":"2025-10-30T22:31:30.898406Z","iopub.status.idle":"2025-10-30T22:31:32.457115Z","shell.execute_reply.started":"2025-10-30T22:31:30.898380Z","shell.execute_reply":"2025-10-30T22:31:32.456398Z"}},"outputs":[{"name":"stdout","text":"Loading model...\nModel ready ✅\n\nLaunching Gradio App...\n* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://6de5f7f3040f766edd.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6de5f7f3040f766edd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":26}]}